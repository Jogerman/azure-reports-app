# apps/reports/tasks.py - Tareas as√≠ncronas con Celery
from celery import shared_task
from django.apps import apps
from django.utils import timezone
import pandas as pd
import numpy as np
import logging
import os

from .utils.specialized_analyzers import get_specialized_analyzer
from .utils.specialized_html_generators import get_specialized_html_generator

logger = logging.getLogger(__name__)

def convert_to_json_serializable(obj):
    """
    Convierte objetos numpy y pandas a tipos serializables en JSON
    """
    if isinstance(obj, dict):
        return {key: convert_to_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_json_serializable(item) for item in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif pd.isna(obj):
        return None
    else:
        return obj

@shared_task
def process_csv_file(csv_file_id):
    """Procesar archivo CSV con an√°lisis real de Azure Advisor"""
    csv_file = None
    temp_file_path = None
    
    try:
        from django.apps import apps
        CSVFile = apps.get_model('reports', 'CSVFile')
        csv_file = CSVFile.objects.get(id=csv_file_id)
        
        logger.info(f"Iniciando procesamiento de CSV {csv_file_id}: {csv_file.original_filename}")
        
        # Actualizar estado
        csv_file.processing_status = 'processing'
        csv_file.save(update_fields=['processing_status'])
        
        # Leer contenido del archivo
        csv_content = None
        if csv_file.azure_blob_url:
            # Si est√° en Azure Storage
            try:
                from apps.storage.services.azure_storage_service import AzureStorageService
                storage_service = AzureStorageService()
                csv_content = storage_service.download_file_content(csv_file.azure_blob_name)
                logger.info(f"Archivo descargado desde Azure Storage: {csv_file.azure_blob_name}")
            except Exception as e:
                logger.warning(f"Error descargando desde Azure Storage: {e}")
        
        if not csv_content and csv_file.file_path:
            # Leer desde archivo local
            try:
                with open(csv_file.file_path, 'r', encoding='utf-8-sig') as f:
                    csv_content = f.read()
                logger.info(f"Archivo le√≠do desde path local: {csv_file.file_path}")
            except Exception as e:
                logger.warning(f"Error leyendo archivo local: {e}")
        
        if not csv_content:
            raise Exception("No se pudo obtener el contenido del archivo CSV")
        
        # **USAR EL NUEVO ANALIZADOR REAL**
        try:
            from apps.reports.analyzers.csv_analyzer import analyze_csv_content
            analysis_results = analyze_csv_content(csv_content)
            logger.info("‚úÖ Usando analizador real de Azure Advisor")
        except ImportError:
            logger.warning("‚ö†Ô∏è  Analizador real no disponible, usando an√°lisis b√°sico")
            # An√°lisis b√°sico como fallback
            import pandas as pd
            from io import StringIO
            
            df = pd.read_csv(StringIO(csv_content))
            analysis_results = {
                'executive_summary': {
                    'total_actions': len(df),
                    'advisor_score': 65,  # Score por defecto
                },
                'cost_optimization': {
                    'estimated_monthly_optimization': len(df) * 100,  # Estimaci√≥n b√°sica
                },
                'totals': {
                    'total_actions': len(df),
                    'total_monthly_savings': len(df) * 100,
                    'total_working_hours': len(df) * 0.5,
                    'azure_advisor_score': 65
                },
                'metadata': {
                    'analysis_date': timezone.now().isoformat(),
                    'csv_rows': len(df),
                    'csv_columns': len(df.columns),
                    'data_source': 'Basic CSV Analysis'
                }
            }
        
        # Guardar resultados
        csv_file.rows_count = analysis_results.get('metadata', {}).get('csv_rows', 0)
        csv_file.columns_count = analysis_results.get('metadata', {}).get('csv_columns', 0)
        csv_file.analysis_data = analysis_results
        csv_file.processing_status = 'completed'
        csv_file.processed_date = timezone.now()
        csv_file.save()
        
        logger.info(f"‚úÖ CSV {csv_file_id} procesado exitosamente: {csv_file.rows_count} filas")
        logger.info(f"üìä Acciones totales: {analysis_results.get('executive_summary', {}).get('total_actions', 0)}")
        logger.info(f"üí∞ Ahorros estimados: ${analysis_results.get('cost_optimization', {}).get('estimated_monthly_optimization', 0):,}")
        
        return f"Procesado exitosamente: {csv_file.rows_count} filas"
        
    except Exception as e:
        error_msg = f"Error procesando CSV {csv_file_id}: {str(e)}"
        logger.error(error_msg, exc_info=True)
        
        if csv_file:
            csv_file.processing_status = 'failed'
            csv_file.error_message = str(e)
            csv_file.save(update_fields=['processing_status', 'error_message'])
        
        # Limpiar archivo temporal en caso de error
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.unlink(temp_file_path)
            except Exception:
                pass
        # Guardar tambi√©n el path del archivo para acceso posterior
        if not csv_file.file_path and temp_file_path:
            csv_file.file_path = temp_file_path
            csv_file.save()

        raise Exception(error_msg)
@shared_task  
def generate_report(report_id):
    """Generar reporte PDF de forma as√≠ncrona - VERSI√ìN ACTUALIZADA"""
    report = None
    
    try:
        Report = apps.get_model('reports', 'Report')
        report = Report.objects.get(id=report_id)
        
        logger.info(f"Iniciando generaci√≥n de reporte {report.id} tipo {report.report_type}")
        
        # Determinar si es especializado o comprehensivo
        if report.report_type in ['security', 'performance', 'cost']:
            # Usar la nueva tarea especializada
            return generate_specialized_report.delay(report_id)
        else:
            # Usar l√≥gica existente para reportes comprehensivos
            return generate_comprehensive_report(report)
            
    except Exception as e:
        error_msg = f"Error determinando tipo de generaci√≥n para reporte {report_id}: {str(e)}"
        logger.error(error_msg, exc_info=True)
        
        if report:
            report.status = 'failed'
            report.error_message = str(e)
            report.save(update_fields=['status', 'error_message'])
        
        raise Exception(error_msg)
    
def generate_comprehensive_report(report):
    """Generar reporte comprehensivo (l√≥gica existente)"""
    try:
        # Aqu√≠ va la l√≥gica existente que ya funciona para reportes comprehensivos
        # Esta es la misma l√≥gica que ya ten√≠as funcionando
        
        logger.info(f"Generando reporte comprehensivo para {report.id}")
        
        # Actualizar estado
        report.status = 'generating'
        report.save(update_fields=['status'])
        
        # Usar el generador existente
        from apps.reports.utils.enhanced_analyzer import EnhancedHTMLReportGenerator
        generator = EnhancedHTMLReportGenerator()
        html_content = generator.generate_complete_html(report)
        
        # Generar PDF
        from apps.storage.services.pdf_generator_service import generate_report_pdf
        pdf_bytes, pdf_filename = generate_report_pdf(report, html_content)
        
        # Subir a Azure Storage
        pdf_url, html_url = upload_report_files_to_azure(
            pdf_bytes, html_content, pdf_filename, report
        )
        
        # Actualizar reporte
        report.pdf_url = pdf_url
        report.html_url = html_url
        report.pdf_blob_name = f"reports/{pdf_filename}"
        report.status = 'completed'
        report.completed_at = timezone.now()
        report.save()
        
        logger.info(f"‚úÖ Reporte comprehensivo completado: {report.id}")
        
        return {
            'report_id': str(report.id),
            'report_type': 'comprehensive',
            'status': 'completed',
            'pdf_url': pdf_url,
            'html_url': html_url
        }
        
    except Exception as e:
        logger.error(f"Error en reporte comprehensivo: {e}", exc_info=True)
        report.status = 'failed'
        report.error_message = str(e)
        report.save(update_fields=['status', 'error_message'])
        raise
    
@shared_task
def generate_specialized_report(report_id):
    """Generar reporte especializado (security, performance, cost) de forma as√≠ncrona"""
    report = None
    
    try:
        Report = apps.get_model('reports', 'Report')
        report = Report.objects.get(id=report_id)
        
        logger.info(f"Iniciando generaci√≥n de reporte especializado {report.id} tipo {report.report_type}")
        
        # Actualizar estado
        report.status = 'processing'
        report.save(update_fields=['status'])
        
        # Validar que sea un tipo especializado
        if report.report_type not in ['security', 'performance', 'cost']:
            raise ValueError(f"Tipo de reporte no especializado: {report.report_type}")
        
        # Obtener datos del CSV
        csv_data = get_csv_dataframe_for_task(report)
        if csv_data is None or csv_data.empty:
            raise ValueError("No se pudieron obtener datos del CSV para el an√°lisis")
        
        logger.info(f"Datos CSV obtenidos: {len(csv_data)} filas para an√°lisis {report.report_type}")
        
        # Ejecutar an√°lisis especializado
        analyzer = get_specialized_analyzer(report.report_type, csv_data)
        analysis_data = analyzer.analyze()
        
        logger.info(f"An√°lisis {report.report_type} completado")
        
        # Generar HTML
        html_generator = get_specialized_html_generator(report.report_type, report, analysis_data)
        html_content = html_generator.generate_html()
        
        logger.info(f"HTML especializado generado")
        
        # Generar PDF
        from apps.storage.services.pdf_generator_service import generate_report_pdf
        pdf_bytes, pdf_filename = generate_report_pdf(report, html_content)
        
        logger.info(f"PDF generado: {len(pdf_bytes)} bytes")
        
        # Subir archivos a Azure Storage
        pdf_url, html_url = upload_report_files_to_azure(
            pdf_bytes, html_content, pdf_filename, report
        )
        
        # Actualizar reporte con resultados
        report.analysis_results = {
            f'{report.report_type}_analysis': analysis_data,
            'generation_metadata': {
                'generated_at': timezone.now().isoformat(),
                'data_source': f'Specialized {report.report_type.title()} Analysis',
                'records_analyzed': len(csv_data),
                'analysis_type': report.report_type
            }
        }
        report.pdf_url = pdf_url
        report.html_url = html_url
        report.pdf_blob_name = f"reports/{pdf_filename}"
        report.html_blob_name = f"reports/{pdf_filename.replace('.pdf', '.html')}"
        report.status = 'completed'
        report.completed_at = timezone.now()
        report.save()
        
        logger.info(f"‚úÖ Reporte especializado {report.report_type} completado: {report.id}")
        
        # Retornar resumen para logging
        dashboard_metrics = analysis_data.get('dashboard_metrics', {})
        return {
            'report_id': str(report.id),
            'report_type': report.report_type,
            'status': 'completed',
            'total_actions': dashboard_metrics.get('total_actions', 0),
            'pdf_url': pdf_url,
            'html_url': html_url
        }
        
    except Exception as e:
        error_msg = f"Error generando reporte especializado {report_id}: {str(e)}"
        logger.error(error_msg, exc_info=True)
        
        if report:
            report.status = 'failed'
            report.error_message = str(e)
            report.save(update_fields=['status', 'error_message'])
        
        raise Exception(error_msg)

def get_csv_dataframe_for_task(report):
    """Obtener DataFrame del CSV para tareas as√≠ncronas"""
    try:
        if not report.csv_file:
            logger.warning("No hay archivo CSV asociado al reporte")
            return None
        
        csv_file = report.csv_file
        
        # M√©todo 1: Desde Azure Blob Storage
        if csv_file.azure_blob_url and csv_file.azure_blob_name:
            try:
                from apps.storage.services.azure_storage_service import AzureStorageService
                storage_service = AzureStorageService()
                csv_content = storage_service.download_file_content(csv_file.azure_blob_name)
                
                # Crear archivo temporal
                import tempfile
                import os
                
                with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as temp_file:
                    temp_file.write(csv_content)
                    temp_path = temp_file.name
                
                # Leer CSV
                df = pd.read_csv(temp_path)
                
                # Limpiar archivo temporal
                os.unlink(temp_path)
                
                logger.info(f"CSV le√≠do desde Azure Storage: {len(df)} filas")
                return df
                
            except Exception as e:
                logger.warning(f"Error leyendo desde Azure Storage: {e}")
        
        # M√©todo 2: Desde analysis_data si est√° disponible
        if csv_file.analysis_data:
            try:
                # Intentar reconstruir DataFrame desde los datos de an√°lisis
                return reconstruct_dataframe_from_analysis(csv_file.analysis_data)
            except Exception as e:
                logger.warning(f"Error reconstruyendo DataFrame: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"Error obteniendo DataFrame para tarea: {e}")
        return None

def reconstruct_dataframe_from_analysis(analysis_data):
    """Reconstruir DataFrame desde datos de an√°lisis existentes"""
    try:
        synthetic_data = []
        
        # M√©todo 1: Si hay raw_data disponible
        if 'raw_data' in analysis_data:
            return pd.DataFrame(analysis_data['raw_data'])
        
        # M√©todo 2: Reconstruir desde category_analysis
        if 'category_analysis' in analysis_data:
            category_counts = analysis_data['category_analysis'].get('counts', {})
            
            business_impacts = ['High', 'Medium', 'Low']
            resource_types = ['Virtual machine', 'Storage Account', 'App Service', 'Virtual machine', 'Subscription']
            
            idx = 0
            for category, count in category_counts.items():
                for i in range(count):
                    synthetic_data.append({
                        'Category': category,
                        'Business Impact': business_impacts[idx % len(business_impacts)],
                        'Recommendation': f'Sample {category.lower()} recommendation #{i+1}',
                        'Resource Type': resource_types[idx % len(resource_types)]
                    })
                    idx += 1
        
        # M√©todo 3: Generar datos b√°sicos desde m√©tricas
        elif 'dashboard_metrics' in analysis_data or 'executive_summary' in analysis_data:
            metrics = analysis_data.get('dashboard_metrics', analysis_data.get('executive_summary', {}))
            total_actions = metrics.get('total_actions', 10)
            
            categories = ['Security', 'Cost', 'Performance', 'Reliability']
            impacts = ['High', 'Medium', 'Low']
            resources = ['Virtual machine', 'Storage Account', 'App Service']
            
            for i in range(min(total_actions, 100)):  # M√°ximo 100 registros sint√©ticos
                synthetic_data.append({
                    'Category': categories[i % len(categories)],
                    'Business Impact': impacts[i % len(impacts)],
                    'Recommendation': f'Synthetic recommendation #{i+1}',
                    'Resource Type': resources[i % len(resources)]
                })
        
        if synthetic_data:
            df = pd.DataFrame(synthetic_data)
            logger.info(f"DataFrame reconstruido con {len(df)} registros sint√©ticos")
            return df
        
        return None
        
    except Exception as e:
        logger.error(f"Error reconstruyendo DataFrame: {e}")
        return None

def upload_report_files_to_azure(pdf_bytes, html_content, pdf_filename, report):
    """Subir archivos de reporte a Azure Storage"""
    try:
        from apps.storage.services.azure_storage_service import AzureStorageService
        storage_service = AzureStorageService()
        
        # Subir PDF
        pdf_blob_name = f"reports/{pdf_filename}"
        pdf_url = storage_service.upload_file_content(pdf_bytes, pdf_blob_name, 'application/pdf')
        
        # Subir HTML
        html_filename = pdf_filename.replace('.pdf', '.html')
        html_blob_name = f"reports/{html_filename}"
        html_bytes = html_content.encode('utf-8')
        html_url = storage_service.upload_file_content(html_bytes, html_blob_name, 'text/html')
        
        logger.info(f"Archivos subidos - PDF: {pdf_url}, HTML: {html_url}")
        return pdf_url, html_url
        
    except Exception as e:
        logger.error(f"Error subiendo archivos a Azure: {e}")
        # En caso de error, retornar None para que el reporte se marque como completado sin URLs
        return None, None
    
@shared_task
def validate_csv_for_specialized_analysis(csv_file_id, report_type):
    """Validar que el CSV tenga los datos necesarios para an√°lisis especializado"""
    try:
        CSVFile = apps.get_model('reports', 'CSVFile')
        csv_file = CSVFile.objects.get(id=csv_file_id)
        
        logger.info(f"Validando CSV {csv_file_id} para an√°lisis {report_type}")
        
        # Obtener DataFrame
        df = get_csv_dataframe_for_task_by_csv_file(csv_file)
        if df is None or df.empty:
            return {'valid': False, 'error': 'No se pudieron cargar datos del CSV'}
        
        # Validaciones espec√≠ficas por tipo
        validation_results = {
            'valid': True,
            'total_records': len(df),
            'columns_found': list(df.columns),
            'type_specific_data': {}
        }
        
        if report_type == 'security':
            security_records = df[df.get('Category', pd.Series()).str.lower() == 'security']
            validation_results['type_specific_data'] = {
                'security_records': len(security_records),
                'has_security_data': len(security_records) > 0
            }
            if len(security_records) == 0:
                validation_results['valid'] = False
                validation_results['error'] = 'No se encontraron registros de seguridad en el CSV'
        
        elif report_type == 'performance':
            performance_records = df[df.get('Category', pd.Series()).str.lower() == 'performance']
            validation_results['type_specific_data'] = {
                'performance_records': len(performance_records),
                'has_performance_data': len(performance_records) > 0
            }
            if len(performance_records) == 0:
                validation_results['valid'] = False
                validation_results['error'] = 'No se encontraron registros de rendimiento en el CSV'
        
        elif report_type == 'cost':
            cost_records = df[df.get('Category', pd.Series()).str.lower() == 'cost']
            validation_results['type_specific_data'] = {
                'cost_records': len(cost_records),
                'has_cost_data': len(cost_records) > 0
            }
            if len(cost_records) == 0:
                validation_results['valid'] = False
                validation_results['error'] = 'No se encontraron registros de costo en el CSV'
        
        logger.info(f"Validaci√≥n completada: {validation_results}")
        return validation_results
        
    except Exception as e:
        logger.error(f"Error validando CSV {csv_file_id}: {e}")
        return {'valid': False, 'error': str(e)}

def get_csv_dataframe_for_task_by_csv_file(csv_file):
    """Helper para obtener DataFrame desde CSVFile object"""
    try:
        if csv_file.azure_blob_url and csv_file.azure_blob_name:
            from apps.storage.services.azure_storage_service import AzureStorageService
            storage_service = AzureStorageService()
            csv_content = storage_service.download_file_content(csv_file.azure_blob_name)
            
            import tempfile
            import os
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as temp_file:
                temp_file.write(csv_content)
                temp_path = temp_file.name
            
            df = pd.read_csv(temp_path)
            os.unlink(temp_path)
            
            return df
        
        elif csv_file.analysis_data:
            return reconstruct_dataframe_from_analysis(csv_file.analysis_data)
        
        return None
        
    except Exception as e:
        logger.error(f"Error obteniendo DataFrame desde CSVFile: {e}")
        return None